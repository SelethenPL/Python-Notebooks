{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "[ZMIO-5] homework-1-frozen-lake.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv9_F8pM7SQe"
      },
      "source": [
        "## Zaawansowane Metody Inteligencji Obliczeniowej"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC1VQKuU7SQg"
      },
      "source": [
        "## Wprowadzenie\n",
        "\n",
        "Całe zadanie jest oparte o różne wersje środowiska `FrozenLake` ze znanej biblioteki OpenAI Gym (https://gym.openai.com), która agreguje różnego rodzaju środowiska pod postacią jednego zunifikowanego API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEjpqVvN7SQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba0a381-de7e-4cdb-8d3f-b76e02cc7385"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq4R-nTJ7SQj"
      },
      "source": [
        "# Zaimportuj środowisko FrozenLake z OpenAI Gym\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv \n",
        "\n",
        "# Stwórzmy deterministyczne (`is_slippper=False`) środowisko w oparciu o jedną z zpredefiniowanych map (`map_name=\"4x4\"`)\n",
        "env = FrozenLakeEnv(map_name=\"4x4\", is_slippery=False) \n",
        "\n",
        "# Po stworzeniu środowiska musimy je zresetować \n",
        "env.reset()\n",
        "# W każdym momencie możemy wyświetlić stan naszego środowiska przy użyciu fukcji `render`\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRD0U6b7SQj"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(\"Przestrzeń akcji: \", env.action_space) # Dyskretne akcje od 0 do 3: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
        "print(\"Przestrzeń obserwacji: \", env.observation_space) # Dyskretne stany od 0 do 15\n",
        "print(\"Opis środowiska (mapa):\")\n",
        "print(env.desc)\n",
        "print(\"Model przejść w środowisku:\")\n",
        "pprint(env.P) # gdzie P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "print(\"Aktualny stan: \", env.s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcBrfUli7SQk"
      },
      "source": [
        "# Nasz agent może wejść w interakcje ze środowiskiem  poprzez wywołanie funkcji `step(action)`, \n",
        "# gdzie `action` to jedna z możliwych akcji (int od 0 do env.action_space.n - 1)\n",
        "s = env.reset() # `reset()` zwraca początkowy stan\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    # Wybierzmy losową akcje\n",
        "    random_a = env.action_space.sample() \n",
        "    # `step(action)` zwraca nowy stan (`s`), nagrodę (`r`), informację czy stan jest terminalny (`term`) \n",
        "    # oraz dodatkowe informacje, które pomijamy\n",
        "    # w tym wypadku nowy stan to jedynie id, ale dla innych środowisk może być to inny typ reprezentujący obserwację\n",
        "    s, r, term, _ = env.step(random_a) \n",
        "    env.render()\n",
        "    if term:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-E847TM7SQm"
      },
      "source": [
        "## Zad 1 - Policy iteration + value iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCfHxGuJ7SQo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_evaluation(policy, P, discount_factor=1.0, delta_min=0.0001):\n",
        "    V = [0] * len(policy)\n",
        "    while(True):\n",
        "        delta = 0\n",
        "        for state in range(len(policy)):\n",
        "            v = V[state]\n",
        "            V[state] = 0\n",
        "            for probability, next_state, reward, is_terminal_state in P[state][policy[state]]:\n",
        "                V[state] += probability * (reward + discount_factor * V[next_state])\n",
        "            delta = max(delta, np.abs(v - V[state]))\n",
        "        if delta < delta_min:\n",
        "            break\n",
        "    return np.array(V)\n",
        "\n",
        "def policy_iteration(P, gamma, delta=0.001):\n",
        "    \"\"\"\n",
        "    Argumenty:\n",
        "        P - model przejścia, gdzie P[s][a] == [(prawdopodobieństwo, kolejny stan, nagroda, czy stan terminalny), ...]\n",
        "        gamma - współczynnik dyskontujący\n",
        "        delta - tolerancja warunku stopu\n",
        "    Zwracane wartości:\n",
        "        V - lista o długości len(P) zawierający oszacowane wartość stanu s: V[s]\n",
        "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
        "        i - ilość iteracji algorytmu po wszystkich stanach\n",
        "    \"\"\"\n",
        "    V = [0] * len(P)\n",
        "    pi = [0] * len(P)\n",
        "    i = 0\n",
        "    \n",
        "    # Miejsce na twoją implementację\n",
        "    policy = np.zeros(len(P), dtype=int)\n",
        "    while True:\n",
        "        V = policy_evaluation(pi, P, discount_factor=gamma)\n",
        "        policy_stable = True\n",
        "        for state in range(len(P)):\n",
        "            chosen_action = pi[state]\n",
        "            action_values = np.zeros(4)\n",
        "            for action in P[state]:\n",
        "                for probability, next_state, reward, is_terminal_state in P[state][action]:\n",
        "                    action_values[action] += probability * (reward + gamma * V[next_state])\n",
        "            best_action = np.argmax(action_values)\n",
        "            if chosen_action != best_action:\n",
        "                policy_stable = False\n",
        "            policy[state] = best_action\n",
        "        i += 1\n",
        "        if policy_stable:\n",
        "            break\n",
        "    return V, pi, i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Cw2lhC7SQp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration(P, gamma, delta=0.0001):\n",
        "    \"\"\"\n",
        "    Argumenty:\n",
        "        P - model przejścia, gdzie P[s][a] == [(prawdopodobieństwo, kolejny stan, nagroda, czy stan terminalny), ...]\n",
        "        gamma - współczynnik dyskontujący\n",
        "        delta - tolerancja warunku stopu\n",
        "    Zwracane wartości:\n",
        "        Q - lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
        "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną politykę - akcję dla stanu s: pi[s]\n",
        "        i - ilość iteracji algorytmu po wszystkich stanach\n",
        "    \"\"\"\n",
        "    pi = [0] * len(P)\n",
        "    Q = [[0] * len(P[s]) for s in P.keys()]\n",
        "    V = [0] * len(P)\n",
        "    i = 0\n",
        "\n",
        "    # Miejsce na twoją implementację\n",
        "    while True:\n",
        "        delta_temp = 0\n",
        "        for state in range(len(P)):\n",
        "            v = V[state]\n",
        "            for action in P[state]:\n",
        "                Q[state][action] = 0\n",
        "                for probablitiy, next_state, reward, is_terminal_state in P[state][action]:\n",
        "                    Q[state][action] += probablitiy * (reward + gamma * V[next_state])\n",
        "            V[state] = max(Q[state])\n",
        "            delta_temp = max(delta_temp, np.abs(v - V[state]))\n",
        "        i += 1\n",
        "        if delta_temp < delta:\n",
        "            break\n",
        "    \n",
        "    for state in range(len(P)):\n",
        "        pi[state] = np.argmax(Q[state])\n",
        "\n",
        "    return Q, pi, i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_zw1szg7SQp"
      },
      "source": [
        "# Przykładowy kod do testowania zaimplementowanych metod\n",
        "\n",
        "# Zaimportuj generator map dla środowiska FrozenLake z OpenAI Gym\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
        "\n",
        "# Wygeneruj losową mapę jeziora o zadanym rozmiarze (`size=`)\n",
        "lake_map = generate_random_map(size=8)\n",
        "\n",
        "# Stwórz środowisko w oparciu o wygenerowaną mapę, \n",
        "# sprawdz deterministyczną (`is_slippery=False`) jak i stochastyczną wersję środowiska (`is_slippery=True`)\n",
        "env = FrozenLakeEnv(desc=lake_map, is_slippery=False) \n",
        "env.render()\n",
        "\n",
        "env = FrozenLakeEnv(map_name=\"8x8\", is_slippery=False) \n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTJ6UBSO7SQq"
      },
      "source": [
        "V, pi1, i1 = policy_iteration(env.P, 0.9)\n",
        "Q, pi2, i2 = value_iteration(env.P, 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHIrfALG7SQr"
      },
      "source": [
        "# Wprowadzmy teraz funkcję, które empirycznie zewauluje naszą politykę\n",
        "# po prostu rozgrywając odpowiednią liczbę episodów zgodnie z naszą polityką.\n",
        "def evaluate_empirically(env, pi, episodes=1000, max_actions=100):\n",
        "    mean_r = 0\n",
        "    for e in range(episodes):\n",
        "        s = env.reset()\n",
        "        total_r = 0\n",
        "        for _ in range(max_actions): # Na wypadek polityki, która nigdy nie dojdzie od stanu terminalnego\n",
        "            s, r, final, _ = env.step(pi[s])\n",
        "            total_r += r\n",
        "            if final:\n",
        "                break\n",
        "        mean_r += 1/(e + 1) * (total_r - mean_r)\n",
        "    return mean_r       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnCEOP5H7SQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eefcf08-2dcf-44c1-976f-fff44bc45413"
      },
      "source": [
        "evaluate_empirically(env, pi2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eUyoahN7SQs"
      },
      "source": [
        "## Zad 2 - On-policy Monte Carlo dla polityki epsilon-greedy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdrtSWJq7SQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada984cd-7343-4889-c91b-f429687f132e"
      },
      "source": [
        "def on_policy_eps_greedy_monte_carlo(env, eps, gamma):\n",
        "    \"\"\"\n",
        "    Argumenty:\n",
        "        env - środowisko implementujące metody `reset()` oraz `step(action)`\n",
        "        eps - współczynnik eksploracji\n",
        "        gamma - współczynnik dyskontujący\n",
        "    Zwracane wartości:\n",
        "        Q - lista o długości len(P) zawierający listy z oszacowanymi wartościami dla stanu s i akcji a: Q[s][a]\n",
        "        pi - lista o długości len(P) zawierający wyznaczoną deterministyczną (zachłanną) politykę - akcję dla stanu s: pi[s]\n",
        "        i - ilość epizodów wygenerowanych przez algorytm\n",
        "    \"\"\"\n",
        "    # Miejsce na twoją implementację\n",
        "\n",
        "    pi = np.ones((env.nS, env.nA))\n",
        "    Q = np.zeros((env.nS, env.nA))\n",
        "    returns_list = [[[] for _ in range(env.nA)] for s in env.P.keys()]\n",
        "\n",
        "    i = 0\n",
        "    while i < 5000:\n",
        "        i += 1\n",
        "\n",
        "        state = env.reset()\n",
        "        episode = []\n",
        "\n",
        "        while True:\n",
        "            probability = [pi[state][action] for action in range(env.nA)]\n",
        "            probability = np.divide(probability, sum(probability))\\\n",
        "\n",
        "            possible_actions = [action for action in range(env.nA)]\n",
        "            action = np.random.choice(possible_actions, p = probability)\n",
        "\n",
        "            next_state, reward, term, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "            if term:\n",
        "                break\n",
        "        \n",
        "        G = 0\n",
        "        for index, (state,action,reward) in enumerate(reversed(episode)):\n",
        "            G = reward + gamma * G\n",
        "\n",
        "            if (state,action) not in [x[0:2] for x in episode[:-index - 1]]:\n",
        "                returns_list[state][action].append(G)\n",
        "                Q[state][action] = np.mean(returns_list[state][action])\n",
        "                A_star = np.argmax(Q[state])\n",
        "\n",
        "                for a in range(env.nA):\n",
        "                    if a == A_star:\n",
        "                        pi[state][a] = 1 - eps + eps / env.nA\n",
        "                    else:\n",
        "                        pi[state][a] = eps / env.nA\n",
        "\n",
        "    return Q, np.argmax(pi, axis=1), i\n",
        "\n",
        "def modify_environment(env):\n",
        "    for state in env.P:\n",
        "        for action in env.P[state]:\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                if reward == 0.0 and done == True:\n",
        "                    env.P[state][action] = [(prob, next_state, -5.0, done)]\n",
        "                if reward == 1.0:\n",
        "                    env.P[state][action] = [(prob, next_state, 5.0, done)]\n",
        "                if reward == 0.0:\n",
        "                    env.P[state][action] = [(prob, next_state, -0.1, done)]\n",
        "\n",
        "Q3, pi3, i3 = on_policy_eps_greedy_monte_carlo(env, 0.9, 0.9)\n",
        "evaluate_empirically(env, pi3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    }
  ]
}